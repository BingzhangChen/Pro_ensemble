
```{r}
library(spatialsample)
library(tidymodels)
library(hardhat)
library(yardstick)
library(dplyr)
tidymodels_prefer()
Pico <- read.csv('pro_data.csv') |>
  select(Depth, Temp, Sal, SSChl, NO3, PO4, Pro, lon, lat, Year, DOY)
```

We run imputation techniques to interpolate the missing values of predictors.

```{r imputation}
library(mice)
Pico_imputed <- mice(Pico, method = "cart")
Pico$Temp <- complete(Pico_imputed)$Temp
Pico$NO3 <- complete(Pico_imputed)$NO3
Pico$PO4 <- complete(Pico_imputed)$PO4
Pico$SSChl <- complete(Pico_imputed)$SSChl
Pico$Sal <- complete(Pico_imputed)$Sal
```

We first need to develop a classification model to predict the presence/absence of each phytoplankton group.

```{r convert_presence}
# Convert Pro to a binary presence/absence variable and ensure it is a factor
Pico <- Pico %>%
  filter(!is.na(Pro)) %>%
  mutate(Pro_presence = factor(if_else(Pro > 0, 1, 0)))

```

```{r test/training_split}
library(sf)
library(dplyr)
library(ggplot2)
Pico_sf <- st_as_sf(Pico, coords = c("lon", "lat"), crs = 4326)

set.seed(121)

candidate_areas <- list(
  area1 = c(xmin = 112, xmax = 120, ymin = 10, ymax = 15),
  area2 = c(xmin = 120, xmax = 130, ymin = 31, ymax = 34)
)

selected_areas <- sample(names(candidate_areas), size = 2)
print(selected_areas)

in_selected_area <- function(lon, lat) {
  any(sapply(selected_areas, function(area) {
    coords <- candidate_areas[[area]]
    lon >= coords["xmin"] & lon <= coords["xmax"] &
      lat >= coords["ymin"] & lat <= coords["ymax"]
  }))
}

coords <- st_coordinates(Pico_sf)
Pico_sf$in_test <- mapply(in_selected_area, coords[,1], coords[,2])

test_data <- Pico_sf %>% filter(in_test)
train_data <- Pico_sf %>% filter(!in_test)

rects <- lapply(selected_areas, function(area) {
  coords <- candidate_areas[[area]]
  st_polygon(list(matrix(
    c(coords["xmin"], coords["ymin"],
      coords["xmax"], coords["ymin"],
      coords["xmax"], coords["ymax"],
      coords["xmin"], coords["ymax"],
      coords["xmin"], coords["ymin"]),
    ncol = 2, byrow = TRUE
  )))
}) %>% st_sfc(crs = 4326)

p <- ggplot() +
  geom_sf(data = train_data, aes(color = "Train"), alpha = 0.6,size = 0.1) +
  geom_sf(data = test_data, aes(color = "Test"), alpha = 0.8, size = 0.1) +
  geom_sf(data = rects, fill = NA, color = "black", linetype = "dashed", size = 0.1) +
  scale_color_manual(values = c("Train" = "steelblue", "Test" = "firebrick")) +
  theme_minimal() +
  labs(title = "Train/Test Split by Fixed Sea Areas",
       color = "Dataset")
p

```

```{r spatial_samlping}
folds <- spatial_clustering_cv(train_data, v = 5)

#Visualize the folds
autoplot(folds)

Pico_split <- folds  

train_df <- st_drop_geometry(train_data)
test_df <- st_drop_geometry(test_data)
```

```{r define_recipe}
# Define recipe without PCA
normalized_rec <- 
  recipe(Pro_presence ~ Depth + NO3 + PO4 + Sal + SSChl + Temp + DOY,
         data = train_df) |>
  step_YeoJohnson(NO3) |>
  step_YeoJohnson(PO4) |>
  step_YeoJohnson(SSChl) |>
  step_normalize(all_predictors())

# Define recipe with PCA
normalized_pca_rec <- 
  recipe(Pro_presence ~ Depth + NO3 + PO4 + Sal + SSChl + Temp + DOY,
         data = train_df) |>
  step_YeoJohnson(NO3) |>
  step_YeoJohnson(PO4) |>
  step_YeoJohnson(SSChl) |>
  step_normalize(all_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = tune()) |>
  step_normalize(all_predictors())

```


```{r model_specification_setup}

linear_reg_spec <- 
  logistic_reg(penalty = tune(), mixture = tune()) %>%
  set_engine('glmnet')

nnet_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_engine('nnet', MaxNWts = 2600) %>%
  set_mode('classification')

svm_r_spec <- 
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_engine('kernlab') %>%
  set_mode('classification')

rf_spec <-
  rand_forest(mtry = tune(), min_n = tune(), trees = tune()) %>%
  set_engine('ranger') %>%
  set_mode('classification')

BRT_spec <-
  boost_tree(mtry  = tune(), min_n = tune(), trees = tune(), learn_rate = tune(), loss_reduction = tune(), stop_iter = tune()) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

```


```{r normalized_wflow_set}
normalized <- 
  workflow_set(
    preproc = list(normalized = normalized_rec,
                   normalized_pca = normalized_pca_rec),
    models  = list(
                   SVM_radial = svm_r_spec,
                   nnet       = nnet_spec)
  )

```


```{r tree_rec}

tree_rec <- 
  recipe(Pro_presence ~ Depth + NO3 + PO4 + Sal + SSChl + Temp + DOY, data = train_df)

tree_rec_pca <- 
  recipe(Pro_presence ~ Depth + NO3 + PO4 + Sal + SSChl + Temp + DOY, data = train_df) %>%
  step_pca(all_numeric_predictors(), threshold = 0.99)

no_pre_proc <- 
  recipe(Pro_presence ~ Depth + NO3 + PO4 + Sal + SSChl + Temp + DOY, data = train_df)

tree_workflows <- 
  workflow_set(
    preproc = list(simple = tree_rec,
                   simple_pca = tree_rec_pca),
    models  = list(RF = rf_spec,
                   BRT = BRT_spec)
  )

```


```{r}
N_feature <- 7
rf_param <-
  rf_spec |>
  extract_parameter_set_dials() |>
  update(
    mtry  = mtry(c(1, N_feature)),
    trees = trees(c(10 * N_feature, 2000)),
    min_n = min_n(c(1, 100))
  )

BRT_param <-
  BRT_spec |>
  extract_parameter_set_dials() |>
  update(
    mtry  = mtry(c(1, N_feature)),
    trees = trees(c(10 * N_feature, 2000)),
    min_n = min_n(c(1, 100))
  )
tree_workflows <- 
  tree_workflows |>
  option_add(param_info = rf_param,  id = "simple_RF") |>
  option_add(param_info = rf_param,  id = "simple_pca_RF") |>
  option_add(param_info = BRT_param, id = "simple_BRT")
```


```{r}
poly_recipe <-
  normalized_rec %>%
  step_poly(all_predictors()) %>%
  step_interact(~ all_predictors():all_predictors())

with_features <-
  workflow_set(
    preproc = list(full_quad = poly_recipe),
    models = list(linear_reg = linear_reg_spec)
  )

```


```{r}
all_workflows <-
  bind_rows(tree_workflows, normalized, with_features) %>%
  mutate(wflow_id = gsub('(simple_)|(normalized_)', '', wflow_id))

```


```{r model_tuning}
#Tuning and evaluating the models
#Apply grid search to each workflow using up to 25 different parameter candidates

#The results show that the option and result columns have been updated
library(yardstick)

# Define the metric set to include f_meas
class_metrics <- metric_set(accuracy, kap, f_meas, roc_auc, pr_auc)
Pico$Pro <- factor(Pico$Pro, levels = c("0", "1"))

grid_ctrl <-
  control_grid(
    save_pred     = TRUE,
    parallel_over = 'everything',
    save_workflow = TRUE
  )

system.time(
    grid_results <-
      all_workflows %>%
      workflow_map(seed      = 1003,
                   resamples = folds,
                   grid      = 50,
                   control   = grid_ctrl,
                   metrics   = class_metrics,
                   verbose   = TRUE)
)

```

Evaluate the cross-validation results for the best single model

```{r cross-validation_roc_auc}
# The rank_results() function will order the models by some performance metric. 
# By default, it uses the first metric in the metric set (accuracy)
grid_results %>%
  rank_results() %>%
  filter(.metric == 'roc_auc') %>%
  select(model, .config, roc_auc = mean, rank)

# Use autoplot to plot the rankings to visualize the best results for each model
autoplot(
  grid_results,
  rank_metric = 'roc_auc', # how to order models
  metric = 'roc_auc', # which metric to visualize
  select_best = TRUE # One point per workflow
) +
  geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +
  lims(y = c(0, 1)) +
  theme(legend.position = 'none') +
  theme_light()

```


```{r cross-validation_f1_score}
grid_results %>%
  rank_results() %>%
  filter(.metric == 'f_meas') %>%
  select(model, .config, f_meas = mean, rank)

# Use autoplot to plot the rankings to visualize the best results for each model based on F1-score
autoplot(
  grid_results,
  rank_metric = 'f_meas', # how to order models by F1-score
  metric = 'f_meas', # which metric to visualize
  select_best = TRUE # One point per workflow
) +
  geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +
  lims(y = c(0, 1)) +
  theme(legend.position = 'none') +
  theme_light()

```

```{r cross-validation_pr_auc}
grid_results %>%
  rank_results() %>%
  filter(.metric == 'pr_auc') %>%
  select(model, .config, pr_auc = mean, rank)

# Use autoplot to plot the rankings to visualize the best results for each model based on F1-score
autoplot(
  grid_results,
  rank_metric = 'pr_auc', # how to order models by F1-score
  metric = 'pr_auc', # which metric to visualize
  select_best = TRUE # One point per workflow
) +
  geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +
  lims(y = c(0, 1)) +
  theme(legend.position = 'none') +
  theme_light()


```

```{r cross-validation_confusion_matrix}
best_model <- grid_results %>%
  rank_results() %>%
  filter(.metric == 'pr_auc') %>%
  top_n(1, wt = mean) %>%
  pull(wflow_id) 


best_model_preds <- grid_results %>%
  filter(wflow_id == best_model) %>% 
  collect_predictions()


cm <- best_model_preds %>%
  conf_mat(truth = Pro_presence, estimate = .pred_class)
print(cm)


p_heatmap <- autoplot(cm, type = "heatmap") + 
  theme_minimal() 
p_mosaic <- autoplot(cm, type = "mosaic") + 
  theme_minimal() 

```

```{r cross-validation_pr_curve}
library(yardstick)

best_model_pr <- best_model_preds %>%
  pr_curve(truth = Pro_presence, .pred_1, event_level = "second")

p_pr <- autoplot(best_model_pr) +
  labs(
    x = "Recall",
    y = "Precision"
  ) +
  theme_light(base_size = 13)
p_pr

```

Evaluate the single best model on the test data (pr_auc, roc_auc, f1_score, confusion_matrix)

```{r}
best_pr <- grid_results %>%
  rank_results() %>%
  filter(.metric == "pr_auc") %>%
  arrange(desc(mean)) %>%
  slice(1)

print(best_pr)

best_id <- best_pr$wflow_id

best_res <- grid_results %>%
  extract_workflow_set_result(best_id)


best_params <- best_res %>%
  select_best(metric = "pr_auc")


final_wf <- grid_results %>%
  extract_workflow(best_id) %>%
  finalize_workflow(best_params)


best_fit <- fit(final_wf, data = train_df)

test_prob  <- predict(best_fit, test_df, type = "prob")
test_class <- predict(best_fit, test_df, type = "class")
test_preds  <- bind_cols(test_prob, test_class) %>%
  bind_cols(test_df %>% select(Pro_presence))

prob_cols <- grep("^\\.pred_", colnames(test_prob), value = TRUE)
print(prob_cols)


if (".pred_1" %in% prob_cols) {
  prob_col <- ".pred_1"
} else {
  prob_col <- prob_cols[1]
}

rocauc_val <- roc_auc(test_preds, truth = Pro_presence, .pred_1, event_level = "second")
f1_val    <- f_meas(test_preds, truth = Pro_presence, estimate = .pred_class)
cm        <- conf_mat(test_preds, truth = Pro_presence, estimate = .pred_class)
prauc_val <- pr_auc(test_preds, truth = Pro_presence, !!sym(prob_col), event_level = "second")

metrics_summary <- bind_rows(
  rocauc_val,
  prauc_val,
  f1_val
)
print(metrics_summary)
print(cm)


```

```{r}

p_heatmap <- autoplot(cm, type = "heatmap") + 
  theme_minimal() 
p_mosaic <- autoplot(cm, type = "mosaic") + 
  theme_minimal() 


p_heatmap
p_mosaic 


```

```{r}
pr_data <- pr_curve(test_preds, truth = Pro_presence, !!sym(prob_col), event_level = "second")

p_pr <- ggplot(pr_data, aes(x = recall, y = precision)) +
  geom_line(color = "black", size = 1.2) +
  geom_point(size = 0.8, color = "black", alpha = 0.8) +
  geom_abline(linetype = "dashed", color = "gray60", linewidth = 0.5) +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(linewidth = 0.3, color = "gray85")
  ) +
  labs(
    x = "Recall",
    y = "Precision"
  )

```

Stack models

```{r}
#Creating the Training set for stacking
## Assemble the assessment set predictions for the training dataset from each candidate model
## First, create an empty data stack using the stacks() function and then add candidate models
collect_notes(grid_results)

library(stacks)
concrete_stack <- 
  stacks() %>%
  add_candidates(grid_results) #includes only the model configurations that have complete results

```


```{r}
#Blend the predictions
#Create a meta-learning model in which the assessment set predictions are the predictors of the observed outcome data
#The most commonly used model is a regularized generalized linear model via the lasso penalty

#It is helpful to constrain the blending coefficients to be nonnegative (Breiman 1996b)
set.seed(1004)
#conflicted::conflicts_prefer(brulee::coef)
ens <- blend_predictions(concrete_stack)

```


```{r}
#Show the details of the meta-learning model
ens
```



```{r}
#Fit the member models
#Update the stacking object with the fitted workflow objects for each member
ens <- fit_members(ens) #this stack model can be used for prediction
```

Evaluate the ensemble model on the test data (pr_auc, roc_auc, f1_score, confusion_matrix)

```{r}
class_metrics <- metric_set(roc_auc)

new_pico <- 
  Pico |>
  dplyr::select(-Pro_presence)
pred <- ens |> 
  predict(test_df, type = 'prob')


```


```{r plot_AUC}

library(Epi)

test_labels <- as.numeric(test_df$Pro_presence) 
test_labels <- test_labels - 1                   

ROC(
  test = pred$.pred_1,      
  stat = test_labels,        
  plot = "ROC"
)

```



```{r}

class_metrics <- metric_set(f_meas)

pred <- ens %>% 
  predict(test_df, type = 'prob')

pred_class <- ifelse(pred$.pred_1 >= 0.5, 1, 0)

test_with_pred <- test_df %>% 
  mutate(predicted = factor(pred_class, levels = c(0, 1)))

f1_score <- class_metrics(test_with_pred, truth = Pro_presence, estimate = predicted)

f1_score

levels(test_with_pred$Pro_presence)

test_with_pred <- bind_cols(test_with_pred, pred)

prauc_val <- pr_auc(test_with_pred, 
                    truth = Pro_presence, 
                    .pred_1, 
                    event_level = "second") 
prauc_val

cm1 <- test_with_pred %>%
  conf_mat(truth = Pro_presence, estimate = predicted)
cm1


p_heatmap <- autoplot(cm1, type = "heatmap") + 
  theme_minimal() 

p_mosaic <- autoplot(cm1, type = "mosaic") + 
  theme_minimal() 

```

```{r}
pr_data <- pr_curve(test_with_pred,
                    truth = Pro_presence,
                    .pred_1,
                    event_level = "second")

head(pr_data)

p_pr <- ggplot(pr_data, aes(x = recall, y = precision)) +
  geom_line(color = "black", size = 1.2) +
  geom_point(size = 0.8, color = "black", alpha = 0.8) +
  geom_abline(linetype = "dashed", color = "gray60", linewidth = 0.5) +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(linewidth = 0.3, color = "gray85")
  ) +
  labs(
    x = "Recall",
    y = "Precision"
  )

print(p_pr)
```

Explain the ensemble model

```{r}
Pico <- Pico %>%
  mutate(Pro_presence = as.numeric(as.character(Pro_presence)))

library(DALEXtra)
vip_features <- normalized_rec$var_info$variable
vip_train <- 
  Pico %>% 
  select(all_of(vip_features))

explainer_ens <- 
  explain_tidymodels(
    ens, 
    data =   vip_train |> select(Depth, Temp, NO3, PO4, SSChl, Sal, DOY), 
    y = Pico |> select(Pro_presence),
    label = "Ensemble",
    verbose = FALSE
)
```


```{r}
#Check the variable importance using the _model_parts()_ function:

model_parts_ens <- model_parts(explainer_ens)
plot(model_parts_ens)

p <- plot(model_parts_ens) +
  #xlim(0, 8) +
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA),
    plot.background  = element_rect(fill = "transparent", colour = NA),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(),
    axis.ticks.x = element_line(colour = "black", size = 0.6),    
    axis.ticks.y = element_line(colour = "black", size = 0.6),   
    axis.ticks.length = unit(0.25, "cm")                          
  )
```


```{r ALE_plots}
#View the Accumulated local dependence profiles 

alp <- model_profile(
  explainer_ens,
  type = 'accumulated',
  N = 100,
  variables = c('NO3')
)
plot(alp)
```

Save models for prediction

```{r save_models}
save(ens, file = 'ens_classification.Rdata')

final_wf <- grid_results %>%
  extract_workflow(best_id) %>%
  finalize_workflow(best_params)

best_fit <- final_wf %>%
  fit(data = train_df)

save(best_fit, file = "BRT_classification.Rdata")
```

